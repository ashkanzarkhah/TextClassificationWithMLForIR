{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "G5zNKo82GizA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0     ID                                               text  \\\n",
      "0               0      0  Bromwell High is a cartoon comedy. It ran at t...   \n",
      "1               1  10000  Homelessness (or Houselessness as George Carli...   \n",
      "2               2  10001  Brilliant over-acting by Lesley Ann Warren. Be...   \n",
      "3               3  10002  This is easily the most underrated film inn th...   \n",
      "4               4  10003  This is not the typical Mel Brooks film. It wa...   \n",
      "...           ...    ...                                                ...   \n",
      "12495       12495   9998  Seeing as the vote average was pretty low, and...   \n",
      "12496       12496   9999  The plot had some wretched, unbelievable twist...   \n",
      "12497       12497    999  I am amazed at how this movie(and most others ...   \n",
      "12498       12498     99  A Christmas Together actually came before my t...   \n",
      "12499       12499      9  Working-class romantic drama from director Mar...   \n",
      "\n",
      "       rating  \n",
      "0           9  \n",
      "1           8  \n",
      "2          10  \n",
      "3           7  \n",
      "4           8  \n",
      "...       ...  \n",
      "12495       9  \n",
      "12496       8  \n",
      "12497      10  \n",
      "12498       8  \n",
      "12499       7  \n",
      "\n",
      "[12500 rows x 4 columns]\n",
      "       Unnamed: 0     ID                                               text  \\\n",
      "0               0      0  Story of a man who has unnatural feelings for ...   \n",
      "1               1  10000  Airport '77 starts as a brand new luxury 747 p...   \n",
      "2               2  10001  This film lacked something I couldn't put my f...   \n",
      "3               3  10002  Sorry everyone,,, I know this is supposed to b...   \n",
      "4               4  10003  When I was little my parents took me along to ...   \n",
      "...           ...    ...                                                ...   \n",
      "12495       12495   9998  Towards the end of the movie, I felt it was to...   \n",
      "12496       12496   9999  This is the kind of movie that my enemies cont...   \n",
      "12497       12497    999  I saw 'Descent' last night at the Stockholm Fi...   \n",
      "12498       12498     99  Some films that you pick up for a pound turn o...   \n",
      "12499       12499      9  This is one of the dumbest films, I've ever se...   \n",
      "\n",
      "       rating  \n",
      "0           3  \n",
      "1           4  \n",
      "2           4  \n",
      "3           1  \n",
      "4           1  \n",
      "...       ...  \n",
      "12495       4  \n",
      "12496       3  \n",
      "12497       3  \n",
      "12498       1  \n",
      "12499       1  \n",
      "\n",
      "[12500 rows x 4 columns]\n",
      "       Unnamed: 0     ID                                               text  \\\n",
      "0               0      0  I went and saw this movie last night after bei...   \n",
      "1               1  10000  Actor turned director Bill Paxton follows up h...   \n",
      "2               2  10001  As a recreational golfer with some knowledge o...   \n",
      "3               3  10002  I saw this film in a sneak preview, and it is ...   \n",
      "4               4  10003  Bill Paxton has taken the true story of the 19...   \n",
      "...           ...    ...                                                ...   \n",
      "12495       12495   9998  I was extraordinarily impressed by this film. ...   \n",
      "12496       12496   9999  Although I'm not a golf fan, I attended a snea...   \n",
      "12497       12497    999  From the start of \"The Edge Of Love\", the view...   \n",
      "12498       12498     99  This movie, with all its complexity and subtle...   \n",
      "12499       12499      9  I've seen this story before but my kids haven'...   \n",
      "\n",
      "       rating  \n",
      "0          10  \n",
      "1           7  \n",
      "2           9  \n",
      "3           8  \n",
      "4           8  \n",
      "...       ...  \n",
      "12495       8  \n",
      "12496      10  \n",
      "12497       8  \n",
      "12498      10  \n",
      "12499       7  \n",
      "\n",
      "[12500 rows x 4 columns]\n",
      "       Unnamed: 0     ID                                               text  \\\n",
      "0               0      0  Once again Mr. Costner has dragged out a movie...   \n",
      "1               1  10000  This is an example of why the majority of acti...   \n",
      "2               2  10001  First of all I hate those moronic rappers, who...   \n",
      "3               3  10002  Not even the Beatles could write songs everyon...   \n",
      "4               4  10003  Brass pictures (movies is not a fitting word f...   \n",
      "...           ...    ...                                                ...   \n",
      "12495       12495   9998  I occasionally let my kids watch this garbage ...   \n",
      "12496       12496   9999  When all we have anymore is pretty much realit...   \n",
      "12497       12497    999  The basic genre is a thriller intercut with an...   \n",
      "12498       12498     99  Four things intrigued me as to this film - fir...   \n",
      "12499       12499      9  David Bryce's comments nearby are exceptionall...   \n",
      "\n",
      "       rating  \n",
      "0           2  \n",
      "1           4  \n",
      "2           1  \n",
      "3           3  \n",
      "4           3  \n",
      "...       ...  \n",
      "12495       1  \n",
      "12496       1  \n",
      "12497       3  \n",
      "12498       3  \n",
      "12499       4  \n",
      "\n",
      "[12500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#First we load our data and check them out\n",
    "import pandas as pd\n",
    "\n",
    "df_train_pos = pd.read_csv(\"Copy of train_pos.csv\", encoding = \"UTF-8\")\n",
    "df_train_neg = pd.read_csv(\"Copy of train_neg.csv\", encoding = \"UTF-8\")\n",
    "df_test_pos = pd.read_csv(\"Copy of test_pos.csv\", encoding = \"UTF-8\")\n",
    "df_test_neg = pd.read_csv(\"Copy of test_neg.csv\", encoding = \"UTF-8\")\n",
    "print(df_train_pos)\n",
    "print(df_train_neg)\n",
    "print(df_test_pos)\n",
    "print(df_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vCD6M9y0Jfo8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text\n",
      "0      Bromwell High is a cartoon comedy. It ran at t...\n",
      "1      Homelessness (or Houselessness as George Carli...\n",
      "2      Brilliant over-acting by Lesley Ann Warren. Be...\n",
      "3      This is easily the most underrated film inn th...\n",
      "4      This is not the typical Mel Brooks film. It wa...\n",
      "...                                                  ...\n",
      "12495  Seeing as the vote average was pretty low, and...\n",
      "12496  The plot had some wretched, unbelievable twist...\n",
      "12497  I am amazed at how this movie(and most others ...\n",
      "12498  A Christmas Together actually came before my t...\n",
      "12499  Working-class romantic drama from director Mar...\n",
      "\n",
      "[12500 rows x 1 columns]\n",
      "                                                    text\n",
      "0      Story of a man who has unnatural feelings for ...\n",
      "1      Airport '77 starts as a brand new luxury 747 p...\n",
      "2      This film lacked something I couldn't put my f...\n",
      "3      Sorry everyone,,, I know this is supposed to b...\n",
      "4      When I was little my parents took me along to ...\n",
      "...                                                  ...\n",
      "12495  Towards the end of the movie, I felt it was to...\n",
      "12496  This is the kind of movie that my enemies cont...\n",
      "12497  I saw 'Descent' last night at the Stockholm Fi...\n",
      "12498  Some films that you pick up for a pound turn o...\n",
      "12499  This is one of the dumbest films, I've ever se...\n",
      "\n",
      "[12500 rows x 1 columns]\n",
      "                                                    text\n",
      "0      I went and saw this movie last night after bei...\n",
      "1      Actor turned director Bill Paxton follows up h...\n",
      "2      As a recreational golfer with some knowledge o...\n",
      "3      I saw this film in a sneak preview, and it is ...\n",
      "4      Bill Paxton has taken the true story of the 19...\n",
      "...                                                  ...\n",
      "12495  I was extraordinarily impressed by this film. ...\n",
      "12496  Although I'm not a golf fan, I attended a snea...\n",
      "12497  From the start of \"The Edge Of Love\", the view...\n",
      "12498  This movie, with all its complexity and subtle...\n",
      "12499  I've seen this story before but my kids haven'...\n",
      "\n",
      "[12500 rows x 1 columns]\n",
      "                                                    text\n",
      "0      Once again Mr. Costner has dragged out a movie...\n",
      "1      This is an example of why the majority of acti...\n",
      "2      First of all I hate those moronic rappers, who...\n",
      "3      Not even the Beatles could write songs everyon...\n",
      "4      Brass pictures (movies is not a fitting word f...\n",
      "...                                                  ...\n",
      "12495  I occasionally let my kids watch this garbage ...\n",
      "12496  When all we have anymore is pretty much realit...\n",
      "12497  The basic genre is a thriller intercut with an...\n",
      "12498  Four things intrigued me as to this film - fir...\n",
      "12499  David Bryce's comments nearby are exceptionall...\n",
      "\n",
      "[12500 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#Now we remove extra columns and just keep their text\n",
    "\n",
    "df_train_pos = df_train_pos.drop('rating', axis=1)\n",
    "df_train_pos = df_train_pos.drop('ID', axis=1)\n",
    "df_train_pos = df_train_pos.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "df_train_neg = df_train_neg.drop('rating', axis=1)\n",
    "df_train_neg = df_train_neg.drop('ID', axis=1)\n",
    "df_train_neg = df_train_neg.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "df_test_pos = df_test_pos.drop('rating', axis=1)\n",
    "df_test_pos = df_test_pos.drop('ID', axis=1)\n",
    "df_test_pos = df_test_pos.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "df_test_neg = df_test_neg.drop('rating', axis=1)\n",
    "df_test_neg = df_test_neg.drop('ID', axis=1)\n",
    "df_test_neg = df_test_neg.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "print(df_train_pos)\n",
    "print(df_train_neg)\n",
    "print(df_test_pos)\n",
    "print(df_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Su2svtF64atl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  sentiment\n",
      "0      Bromwell High is a cartoon comedy. It ran at t...          1\n",
      "1      Homelessness (or Houselessness as George Carli...          1\n",
      "2      Brilliant over-acting by Lesley Ann Warren. Be...          1\n",
      "3      This is easily the most underrated film inn th...          1\n",
      "4      This is not the typical Mel Brooks film. It wa...          1\n",
      "...                                                  ...        ...\n",
      "24995  Towards the end of the movie, I felt it was to...         -1\n",
      "24996  This is the kind of movie that my enemies cont...         -1\n",
      "24997  I saw 'Descent' last night at the Stockholm Fi...         -1\n",
      "24998  Some films that you pick up for a pound turn o...         -1\n",
      "24999  This is one of the dumbest films, I've ever se...         -1\n",
      "\n",
      "[25000 rows x 2 columns]\n",
      "                                                    text  sentiment\n",
      "0      I went and saw this movie last night after bei...          1\n",
      "1      Actor turned director Bill Paxton follows up h...          1\n",
      "2      As a recreational golfer with some knowledge o...          1\n",
      "3      I saw this film in a sneak preview, and it is ...          1\n",
      "4      Bill Paxton has taken the true story of the 19...          1\n",
      "...                                                  ...        ...\n",
      "24995  I occasionally let my kids watch this garbage ...         -1\n",
      "24996  When all we have anymore is pretty much realit...         -1\n",
      "24997  The basic genre is a thriller intercut with an...         -1\n",
      "24998  Four things intrigued me as to this film - fir...         -1\n",
      "24999  David Bryce's comments nearby are exceptionall...         -1\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we merge our train data and our test data together and also add\n",
    "# a sentiment column as our label\n",
    "\n",
    "df_train_pos['sentiment'] = 1\n",
    "df_train_neg['sentiment'] = -1\n",
    "df_test_pos['sentiment'] = 1\n",
    "df_test_neg['sentiment'] = -1\n",
    "\n",
    "df_train = pd.concat([df_train_pos, df_train_neg], ignore_index=True)\n",
    "df_test = pd.concat([df_test_pos, df_test_neg], ignore_index=True)\n",
    "\n",
    "print(df_train)\n",
    "print(df_test)\n",
    "\n",
    "import gc\n",
    "\n",
    "del df_train_pos\n",
    "del df_train_neg\n",
    "del df_test_pos\n",
    "del df_test_neg\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pN3lhf86KOOZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ashkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Now we develop a tokenizer function so we use it both on train and test documents\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def tokenizer(document):\n",
    "    # First we lower all our words\n",
    "    document = document.lower()\n",
    "\n",
    "    # Now we replace shortened forms back to their original form\n",
    "    shortened_words_dic = {\n",
    "        '\\n' : ' ',\n",
    "        'i.e.': 'that is',\n",
    "        'e.g.': 'for example',\n",
    "        'won’t': ' will not',\n",
    "        'can’t': ' can not',\n",
    "        '’ve': ' have',\n",
    "        '’m': ' am',\n",
    "        '’d': ' would',\n",
    "        '’ll': ' will',\n",
    "        '’s': ' is',\n",
    "        '’re': ' are',\n",
    "        'n’t': ' not'\n",
    "    }\n",
    "    for key, value in shortened_words_dic.items():\n",
    "        document = document.replace(key, value)\n",
    "\n",
    "    # Now we tokenize it with nltk word tokenizer\n",
    "    tokenized_document = word_tokenize(document)\n",
    "\n",
    "    # Now we remove all punctuation marks or tokens made of only punctuation marks from our tokens\n",
    "    bad_tokens = [',', '?', '”', ')', '’', ';', '“', '.', '$', '(', ':', '!', '/', '-', '[', ']', '{', '}', \"'\"]\n",
    "    tokenized_document = [\n",
    "        token for token in tokenized_document if token not in bad_tokens]\n",
    "\n",
    "    new_tokenized_document = []\n",
    "    for token in tokenized_document:\n",
    "        flag = False\n",
    "        for char in token:\n",
    "            if(char not in bad_tokens):\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            new_tokenized_document.append(token)\n",
    "    tokenized_document = new_tokenized_document\n",
    "\n",
    "    # Now we remove punctuation marks that are left in the middle or at the end of tokens\n",
    "    tokenized_document = [token.rstrip('.') for token in tokenized_document]\n",
    "    tokenized_document = [token.rstrip('?') for token in tokenized_document]\n",
    "    tokenized_document = [token.rstrip('—') for token in tokenized_document]\n",
    "    tokenized_document = [token.rstrip('-') for token in tokenized_document]\n",
    "    tokenized_document = [token.rstrip('–') for token in tokenized_document]\n",
    "    tokenized_document = [token.rstrip('/') for token in tokenized_document]\n",
    "\n",
    "    new_tokenized_document = []\n",
    "    for token in tokenized_document:\n",
    "        if '.' in token:\n",
    "            cur = token.split('.')\n",
    "            for i in range(len(cur)):\n",
    "                if(i + 1 == len(cur)):\n",
    "                    new_tokenized_document.append(cur[i])\n",
    "                else:\n",
    "                    if(len(cur[i]) == 0):\n",
    "                        continue\n",
    "                    if(cur[i][0] <= '9' and len(cur[i + 1]) > 0 and cur[i + 1][0] <= '9'):\n",
    "                        cur[i + 1] = cur[i] + cur[i + 1]\n",
    "                        continue\n",
    "                    new_tokenized_document.append(cur[i])\n",
    "        elif '?' in token:\n",
    "            new_tokenized_document.extend(token.split('?'))\n",
    "        elif '-' in token:\n",
    "            new_tokenized_document.extend(token.split('-'))\n",
    "        elif '—' in token:\n",
    "            new_tokenized_document.extend(token.split('—'))\n",
    "        elif '–' in token:\n",
    "            new_tokenized_document.extend(token.split('–'))\n",
    "        elif \"'\" in token:\n",
    "            new_tokenized_document.extend(token.split(\"'\"))\n",
    "        elif \"`\" in token:\n",
    "            new_tokenized_document.extend(token.split(\"`\"))\n",
    "        elif \"/\" in token:\n",
    "            new_tokenized_document.extend(token.split(\"/\"))\n",
    "        elif \"<\" in token:\n",
    "            new_tokenized_document.extend(token.split(\"<\"))\n",
    "        elif \">\" in token:\n",
    "            new_tokenized_document.extend(token.split(\">\"))\n",
    "        else:\n",
    "            new_tokenized_document.append(token)\n",
    "    tokenized_document = new_tokenized_document\n",
    "\n",
    "    #Now we remove empty tokens generated from filtering punctuation marks\n",
    "    tokenized_document = [\n",
    "        token for token in tokenized_document if len(token) != 0]\n",
    "\n",
    "    # Now we remove stop words from it\n",
    "    stop_words = ['he', 'for', 'in', 'is',\n",
    "                  'was', 'of', 'and', 'to', 'a', 'the']\n",
    "    tokenized_document = [\n",
    "        token for token in tokenized_document if token not in stop_words]\n",
    "\n",
    "    # Now its time to convert our tokens to their stemmed forms\n",
    "    stemmed_tokenized_document = [porter_stemmer.stem(\n",
    "        token) for token in tokenized_document]\n",
    "\n",
    "    # Now that we are done with our tokenizing and preprocessing the tokens, it's time to return them\n",
    "    return stemmed_tokenized_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XXq73w4lLeZr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52885\n"
     ]
    }
   ],
   "source": [
    "#Now we tokenize all our train data and find out how many term do we have\n",
    "all_tokens = []\n",
    "for i in range(df_train.shape[0]):\n",
    "  all_tokens.extend(tokenizer(df_train.iloc[i]['text']))\n",
    "\n",
    "terms = list(set(all_tokens))\n",
    "print(len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oMlGjCOtLudL"
   },
   "outputs": [],
   "source": [
    "#Now because our tf_idf matrice will get very sparce\n",
    "#we count each terms frequency and filter very sparce terms\n",
    "tf = {}\n",
    "for token in all_tokens:\n",
    "  if(token in tf.keys()):\n",
    "    tf[token] += 1\n",
    "  else:\n",
    "    tf[token] = 1\n",
    "\n",
    "tf_term = [(tf[term], term) for term in terms]\n",
    "tf_term.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248, 'section')\n"
     ]
    }
   ],
   "source": [
    "print(tf_term[-2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "apCyKcxpLzbt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'br', 'i', 'thi', 'that', 's', 'movi', 'film', 'as', 'with', 'but', 't', 'you', 'on', 'be', 'n', 'not', 'have', 'are', 'hi', 'one', 'all', 'at', 'they', 'like', 'by', 'an', 'who', 'so', 'from', 'do', 'there', 'her', 'or', 'just', 'about', 'ha', 'out', 'if', 'what', 'time', 'some', 'good', 'make', 'more', 'she', 'when', 'charact', 'get', 'see', 'veri', 'watch', 'up', 'would', 'stori', 'even', 'no', 'my', 'can', 'which', 'onli', 'realli', 'had', 'their', 'were', 'well', 'we', 'me', 'other', 'scene', 'did', 'doe', 'look', 'than', 'show', 'much', 'end', 'will', 'could', 'peopl', 'bad', 'go', 'been', 'great', 'also', 'into', 'first', 'becaus', 'love', 'think', 'how', 'him', 'way', 'act', 'most', 'play', 'made', 'thing', 'then', 'them', 'too', 'ani', 'after', 'know', 'say', 'seem', 'work', '*', 'plot', 'two', 'actor', 'year', 'come', 'mani', 'seen', 'take', 'life', 'want', 'never', 'littl', 'best', 'where', 'over', 'tri', 'off', 'man', 'ever', 'here', 'give', 'better', 'your', 'still', 'perform', 'find', 'these', 'should', 'whi', 'while', 'feel', 'part', 've', 'such', 'back', 'use', 'someth', 'director', 'actual', 'm', 'through', 'interest', 'lot', 'real', 'those', 'now', 'old', 'cast', 'though', 're', 'live', 'star', 'enjoy', 'guy', 'anoth', 'befor', 'new', 'role', 'noth', 'funni', 'music', 'point', '10', 'start', 'set', 'few', 'girl', 'same', 'origin', 'day', 'again', 'everi', 'world', 'believ', 'turn', 'quit', '&', 'direct', 'down', 'us', 'thought', 'fact', 'minut', 'got', 'kill', 'horror', 'action', 'comedi', 'pretti', 'young', 'wonder', 'happen', 'around', 'effect', 'right', 'ca', 'long', 'howev', 'own', 'big', 'line', 'famili', 'enough', 'seri', 'both', 'need', 'may', 'between', 'fan', 'bit', 'script', 'beauti', 'person', 'becom', 'without', 'must', 'alway', 'friend', 'tell', 'reason', 'saw', 'last', 'final', 'kid', 'almost', 'put', 'least', 'sure', 'done', 'whole', 'place', 'complet', 'expect', 'kind', 'differ', 'shot', 'd', 'far', 'mean', 'anyth', 'book', 'laugh', 'might', 'name', 'sinc', 'begin', 'll', 'probabl', 'woman', 'entertain', 'help', 'let', 'screen', 'call', 'moment', 'tv', 'away', 'am', '2', 'read', 'yet', 'rather', 'worst', 'run', 'fun', 'lead', 'hard', 'audienc', 'idea', 'anyon', 'episod', 'american', 'found', 'each', 'appear', 'bore', 'especi', 'our', 'although', 'hope', 'cours', 'keep', 'anim', 'job', 'goe', 'move', 'sens', 'version', 'dvd', 'war', 'money', 'someon', 'mind', 'mayb', 'problem', 'true', 'onc', 'hous', 'everyth', 'nice', 'second', 'three', 'follow', 'recommend', 'face', 'night', 'rate', 'main', 'product', 'worth', 'leav', 'human', 'special', 'excel', 'togeth', 'sound', 'everyon', 'wast', 'john', 'hand', 'eye', 'father', 'later', 'said', 'view', 'instead', 'review', 'high', 'boy', 'hour', 'talk', 'miss', 'classic', 'himself', 'wife', 'understand', 'dure', 'left', 'care', 'black', 'death', 'open', 'murder', 'write', 'half', 'head', 'rememb', 'chang', 'viewer', 'fight', 'surpris', 'gener', 'short', 'includ', 'die', 'els', 'fall', 'less', 'entir', 'piec', 'involv', '1', 'pictur', 'simpli', 'top', 'power', 'home', 'total', 'usual', 'attempt', 'budget', 'suppos', 'releas', 'hollywood', 'terribl', 'song', 'men', 'possibl', 'portray', 'featur', 'disappoint', 'poor', 'coupl', 'stupid', 'camera', 'dead', 'wrong', 'low', 'produc', 'video', 'either', 'aw', 'definit', 'except', 'rest', 'given', 'absolut', 'women', 'lack', 'word', 'writer', '3', 'talent', 'decid', 'until', 'full', 'titl', 'perfect', 'along', 'style', 'close', 'truli', 'school', 'save', 'emot', 'sex', 'age', 'next', 'bring', 'mr', 'case', 'killer', 'comment', 'heart', 'sort', 'perhap', 'creat', 'came', 'exampl', 'sever', 'brother', 'joke', 'game', 'dialogu', 'art', 'small', 'base', 'flick', 'written', 'sequenc', 'meet', 'often', 'mother', 'develop', 'humor', 'earli', 'actress', 'consid', 'dark', 'guess', 'unfortun', 'amaz', 'itself', 'light', 'cinema', 'lost', 'ye', 'drama', 'white', 'experi', 'imagin', 'mention', 'stop', 'natur', 'forc', 'manag', 'felt', 'present', 'cut', 'children', 'fail', 'qualiti', 'support', 'son', 'car', 'ask', 'hit', 'side', 'voic', 'against', 'extrem', 'impress', 'wors', 'evil', 'went', 'stand', 'certainli', 'basic', 'oh', 'overal', 'favorit', 'horribl', 'mysteri', 'number', 'type', 'danc', 'wait', 'hero', 'alreadi', 'learn', 'matter', 'under', 'michael', 'genr', 'fine', 'despit', 'throughout', 'walk', 'success', '\\x96', 'histori', 'question', 'zombi', 'relationship', 'realiz', 'past', 'child', 'town', 'daughter', 'late', 'wish', 'hate', 'event', 'credit', 'theme', 'touch', 'citi', 'today', 'sometim', 'behind', 'god', 'twist', 'b', 'annoy', 'sit', 'deal', 'stay', 'abl', 'rent', '4', 'pleas', 'edit', 'blood', 'deserv', 'anyway', 'comic', 'appar', 'soon', 'gave', 'etc', 'level', 'slow', 'chanc', 'score', 'wo', 'bodi', 'brilliant', 'incred', 'figur', 'situat', 'major', 'themselv', 'self', 'stuff', 'decent', 'element', 'return', 'dream', 'obvious', 'continu', 'order', 'ridicul', 'pace', 'happi', 'highli', 'thank', 'myself', 'add', 'group', 'ladi', 'novel', 'speak', 'pain', 'strang', 'career', 'shoot', 'heard', 'sad', 'polic', 'import', 'husband', 'break', 'took', 'strong', 'predict', 'robert', 'violenc', 'recent', 'hilari', '5', 'countri', 'known', 'pick', 'particularli', 'documentari', 'season', 'critic', 'jame', 'compar', 'obviou', 'alon', 'told', 'state', 'visual', 'theater', 'offer', 'opinion', 'exist', 'rock', 'hold', 'gore', 'crap', 'result', 'hear', 'realiti', 'room', 'effort', 'thriller', 'caus', 'sequel', 'explain', 'serious', 'king', 'local', 'ago', 'hell', 'none', 'note', 'david', 'allow', 'sister', 'femal', 'simpl', 'deliv', 'ok', 'check', 'convinc', 'class', 'suspens', 'win', 'buy', 'oscar', 'huge', 'sexual', 'valu', 'scari', 'cool', 'similar', 'excit', 'provid', 'exactli', 'avoid', 'spoiler', 'shown', 'apart', 'yourself', 'seriou', 'taken', 'whose', 'english', 'cinematographi', 'polit', 'shock', 'offic', 'across', 'middl', 'street', 'somewhat', 'silli', 'messag', 'modern', 'charm', 'pass', 'filmmak', 'confus', 'form', 'tale', 'singl', 'jack', 'sing', 'william', 'mostli', 'attent', 'prove', 'subject', 'richard', 'five', 'carri', 'stage', 'georg', 'team', 'unlik', 'televis', 'cop', 'monster', 'earth', 'villain', 'cover', 'pay', 'marri', 'toward', 'build', 'pull', 'parent', 'respect', 'due', 'fill', 'dialog', 'remind', 'four', 'futur', 'weak', 'typic', 'intellig', 'cheap', 'british', 'atmospher', 'clearli', 'knew', 'dog', 'paul', 'fast', 'artist', 'crime', 'easili', 'non', 'escap', 'adult', 'doubt', 'detail', 'date', 'romant', 'member', 'fire', '80', 'drive', 'gun', 'straight', 'fit', 'beyond', 'attack', 'upon', 'posit', 'imag', 'whether', 'peter', 'fantast', 'aspect', 'captur', 'appreci', 'ten', 'plan', 'discov', 'o', 'remain', 'period', 'realist', 'near', 'air', 'mark', 'adapt', 'red', 'dull', 'within', 'spend', 'materi', 'lose', 'color', 'mari', 'forget', 'chase', 'storylin', 'bunch', 'abov', 'clear', 'match', 'lee', 'victim', 'nearli', 'box', 'york', 'inspir', 'herself', 'finish', 'mess', 'standard', 'easi', 'truth', 'busi', 'suffer', 'bill', 'western', 'space', 'dramat', 'battl', 'notic', 'list', 'de', 'french', '8', 'ad', 'tom', 'larg', 'eventu', 'among', 'train', 'accept', 'agre', 'spirit', 'soundtrack', 'teenag', 'third', 'soldier', 'adventur', 'famou', 'suggest', 'sorri', 'drug', 'cri', 'ultim', 'troubl', 'normal', 'babi', '7', 'certain', 'contain', 'cultur', 'romanc', 'rare', 'lame', 'somehow', 'mix', 'disney', 'gone', 'student', 'cartoon', 'reveal', 'fear', 'suck', 'kept', 'attract', 'appeal', 'premis', 'greatest', 'design', 'secret', 'shame', 'throw', 'copi', 'scare', 'wit', 'america', 'admit', 'relat', 'brought', 'particular', 'screenplay', '70', 'whatev', 'pure', 'averag', 'master', 'harri', 'describ', 'treat', 'male', 'issu', 'warn', 'inde', 'background', 'forward', 'project', 'free', 'fantasi', 'okay', 'japanes', 'poorli', 'memor', 'locat', 'award', 'amus', 'potenti', 'struggl', 'societi', 'magic', 'weird', '9', 'express', 'doctor', 'accent', 'water', '20', 'odd', 'hot', 'choic', 'studio', 'masterpiec', 'imdb', 'crazi', 'control', 'alien', 'becam', 'fiction', 'dr', 'fli', 'difficult', 'joe', 'scream', 'refer', 'uniqu', 'lover', 'costum', 'remak', 'nor', 'vampir', 'girlfriend', 'prison', 'execut', 'wear', 'jump', 'wood', 'unless', 'creepi', 'cheesi', 'superb', 'otherwis', 'parti', 'ghost', 'roll', 'public', 'mad', '30', 'earlier', 'depict', 'badli', 'moral', 'jane', 'week', 'flaw', 'grow', 'dumb', 'maker', 'deep', 'footag', 'connect', 'cliché', 'bother', 'plenti', 'outsid', 'older', 'cat', 'fi', 'stick', 'gay', 'catch', 'whom', 'plu', 'popular', 'equal', 'disturb', 'social', 'quickli', 'sci', 'perfectli', 'dress', 'era', 'mistak', 'ride', 'previou', 'lie', 'combin', 'co', 'concept', 'band', 'surviv', 'rich', 'answer', '90', 'front', 'christma', 'insid', 'eat', 'concern', 'bare', 'sweet', 'listen', 'ben', 'beat', 'term', 'serv', 'meant', 'stereotyp', 'german', 'la', 'hardli', 'law', 'innoc', 'desper', 'promis', 'memori', 'variou', 'cute', 'steal', 'intent', 'inform', 'brain', 'tone', 'nuditi', 'island', 'post', 'amount', 'track', 'compani', 'store', 'flat', 'claim', 'hair', 'univers', 'land', 'kick', 'further', 'scott', 'fairli', 'danger', 'player', 'step', 'plain', 'crew', 'toni', 'share', 'tast', 'centuri', 'achiev', 'record', 'engag', 'cold', 'travel', 'suit', 'wrote', 'tension', 'sadli', 'rip', 'manner', 'spot', 'intens', 'fascin', 'familiar', 'histor', 'remark', 'depth', 'burn', 'destroy', 'sleep', 'languag', 'c', 'purpos', 'ruin', 'ignor', 'unbeliev', 'delight', 'italian', 'abil', 'soul', 'detect', 'collect', 'clever', 'violent', 'reach', 'rape', 'door', 'liter', 'trash', 'commun', 'caught', 'scienc', 'reveng', 'creatur', 'trip', 'approach', 'intrigu', 'fashion', 'skill', 'paint', 'introduc', 'complex', 'channel', 'camp', 'christian', 'hole', 'extra', 'comput', 'mental', 'limit', 'immedi', 'ann', 'slightli', 'million', 'mere', 'teen', 'slasher', 'conclus', 'suddenli', 'spent', 'neither', 'nation', 'imposs', 'crimin', 'respons', 'physic', '50', 'fake', 'planet', 'receiv', 'blue', 'sick', 'bizarr', 'indian', 'embarrass', 'drop', '15', 'ring', 'pop', 'drag', 'haunt', 'pointless', 'suspect', 'search', 'handl', 'edg', 'common', 'biggest', 'faith', 'technic', 'hurt', 'arriv', 'angel', 'genuin', 'dad', 'solid', 'former', 'awesom', 'van', 'colleg', 'focu', 'count', 'tear', 'laughabl', 'visit', 'rais', 'heavi', 'younger', 'sign', 'fair', 'excus', 'wall', 'key', 'stun', 'motion', 'cult', 'tough', 'desir', 'addit', 'super', 'smith', 'cloth', 'davi', 'tortur', 'exploit', 'race', 'author', 'cross', 'focus', 'consist', 'compel', 'pathet', 'minor', 'jim', 'commit', 'chemistri', 'tradit', 'frank', 'won', 'park', 'obsess', 'grade', 'asid', 'steve', 'f', 'brutal', 'somewher', 'rule', 'opportun', 'explor', 'depress', 'honest', 'grant', 'dub', 'besid', 'u', 'trailer', 'intend', 'west', 'regard', 'longer', 'judg', 'scientist', 'decad', 'bar', 'silent', 'creativ', 'anti', 'wild', 'stewart', 'south', 'armi', 'draw', '6', 'road', 'boss', 'govern', 'practic', 'surprisingli', 'motiv', 'gang', 'club', 'redeem', 'page', 'london', 'festiv', 'ex', 'aliv', 'machin', 'green', 'display', 'repeat', 'militari', 'idiot', 'folk', 'yeah', 'thrill', 'nobodi', 'garbag', 'smile', 'journey', 'tire', 'ground', 'mood', 'cost', 'bought', '60', 'stone', 'sam', 'noir', 'mouth', 'terrif', 'honestli', 'agent', 'utterli', 'sexi', 'requir', 'geniu', 'area', 'report', 'enter', 'investig', 'humour', 'glad', 'serial', 'passion', 'occasion', 'narr', 'marriag', 'climax', 'studi', 'industri', 'ship', 'nowher', 'charli', 'center', '%', 'wow', 'loos', '100', 'hors', 'hang', 'demon', 'bear', 'graphic', '40', 'admir', 'giant', 'loud', 'damn', 'subtl', 'send', 'rel', 'profession', 'nake', 'bottom', 'blow', 'kelli', 'insult', 'doubl', 'batman', 'boyfriend', 'initi', 'frame', 'drawn', 'gem', 'opera', 'cinemat', 'church', 'challeng', 'affect', 'seek', 'nightmar', 'fulli', 'evid', 'essenti', 'conflict', 'arm', 'henri', 'grace', 'christoph', 'wind', 'narrat', 'assum', 'witch', 'push', 'wise', 'hunt', 'nomin', 'month', 'chri', 'sceneri', 'repres', 'hide', 'avail', 'affair', 'smart', 'outstand', 'justic', 'interview', 'bond', 'thu', 'satisfi', 'presenc', 'flashback', 'constantli', 'central', 'sell', 'iron', 'content', 'bed', 'gag', 'everybodi', 'slowli', 'hotel', 'hire', 'system', 'hey', 'adam', 'thrown', 'mediocr', 'lesson', 'individu', 'charl', 'jone', 'allen', 'photographi', 'billi', 'cameo', 'ray', 'pari', 'fellow', 'strike', 'rise', 'independ', 'brief', 'absurd', 'neg', 'model', 'phone', 'impact', 'ill', 'born', 'ahead', 'spoil', 'likabl', 'don', 'angl', 'hill', 'fresh', 'abus', 'segment', 'sight', 'sent', 'r', 'photograph', 'discuss', 'shine', 'occur', 'logic', 'blame', 'mainli', 'forev', 'skip', 'commerci', 'bruce', 'teacher', 'surround', 'na', 'held', 'zero', 'blond', 'satir', 'trap', 'summer', 'six', 'resembl', 'fool', 'tragedi', 'queen', 'j', 'bomb', 'ball', 'twice', 'pack', 'reaction', 'hospit', 'protagonist', 'sub', 'sport', 'mile', 'vote', 'trust', 'jerri', 'drink', 'mom', 'encount', 'plane', 'station', 'program', 'current', 'martin', 'choos', 'celebr', 'lord', 'join', 'al', '#', 'tragic', 'field', 'favourit', 'vision', 'round', 'tie', 'robot', 'jean', 'arthur', 'roger', 'psycholog', 'fortun', 'random', 'nonsens', 'intern', 'dread', 'prefer', 'improv', 'epic', 'pleasur', 'highlight', 'formula', 'legend', 'gorgeou', 'dollar', 'wide', 'thin', 'tape', 'porn', 'object', 'fox', 'ugli', 'influenc', 'buddi', 'prepar', 'nasti', 'l', 'warm', 'supposedli', 'reflect', 'progress', 'worthi', 'ii', 'youth', 'unusu', 'length', 'latter', '11', 'superior', 'crash', 'shop', 'childhood', 'theatr', 'seven', 'remot', 'funniest', 'paid', 'disgust', 'convers', 'trick', 'pilot', 'fell', 'establish', 'castl', 'rob', 'disast', 'suicid', 'mine', 'ident', 'disappear', 'heaven', 'gangster', 'forgotten', 'tend', 'singer', 'mask', 'heroin', 'partner', 'decis', 'brian', 'recogn', 'desert', 'alan', 'thoroughli', 'stuck', 'ms', 'replac', 'accur', 'seemingli', 'market', 'eddi', 'commentari', 'andi', 'uncl', 'sky', 'danni', 'clue', 'jackson', 'devil', 'therefor', 'refus', 'pair', 'unit', 'river', 'fault', 'accid', 'tune', 'fate', 'afraid', 'stephen', 'russian', 'quick', 'hidden', 'ed', 'clean', 'test', 'readi', 'irrit', 'instanc', 'convey', 'captain', 'insan', 'frustrat', 'european', 'wed', 'rescu', 'food', 'dirti', 'daniel', 'lock', 'chines', 'angri', 'joy', 'bland', '1950', 'steven', 'price', 'cage', 'rang', 'anymor', 'wooden', 'jason', 'rush', 'news', 'worri', 'twenti', 'martial', 'led', 'board', 'transform', 'hunter', 'symbol', 'sentiment', 'piti', 'onto', 'johnni', 'invent', 'cgi', 'process', 'explan', 'attitud', 'owner', 'x', 'awar', 'necessari', 'floor', 'favor', 'energi', 'target', 'religi', 'p', 'opposit', 'window', 'movement', 'insight', 'blind', 'deepli', 'chick', 'whatsoev', 'research', 'mountain', 'shadow', 'rain', 'possess', 'comparison', 'comed', 'began', 'parodi', 'grand', 'bank', 'princ', 'mid', 'credibl', 'weapon', 'taylor', 'friendship', 'aim', 'teach', 'hint', 'flesh', 'terror', 'protect', 'dougla', 'bloodi', '12', 'pre', 'marvel', 'freddi', 'superman', 'load', 'leader', 'drunk', 'brown', 'anybodi', 'accord', 'watchabl', 'seat', 'jeff', 'hitler', 'appropri', 'unknown', 'tim', 'knock', 'empti', 'charg', 'villag', 'unnecessari', 'england', 'wave', 'utter', 'strength', 'perspect', 'keaton', 'enemi', 'dare', 'craft', 'buck', 'soap', 'nativ', 'media', 'knowledg', 'kiss', 'g', 'ford', 'contrast', 'speed', 'nazi', 'magnific', 'distract', 'correct', 'chill', 'anywher', 'mission', 'ice', 'fred', 'breath', '1980', 'kate', 'jr', 'joan', 'crowd', 'moon', 'frighten', 'soft', 'nick', 'hundr', 'dick', 'somebodi', 'simon', 'radio', 'dozen', 'shakespear', 'loss', 'e', 'dan', 'andrew', 'academi', 'vehicl', 'thousand', 'root', 'quot', 'account', 'leg', 'behavior', 'regular', 'sum', 'pretenti', 'gold', 'demand', 'convent', 'compet', 'worker', 'stretch', 'privat', 'notabl', 'japan', 'interpret', 'candi', '1970', 'tarzan', 'prais', 'lynch', 'explos', 'debut', 'constant', 'translat', 'spi', 'sea', 'revolv', 'threaten', 'technolog', 'sat', 'quiet', 'met', 'failur', 'abandon', 'vh', 'mike', 'kevin', 'jesu', 'higher', 'ass', 'aid', 'punch', 'interact', 'franc', 'toy', 'separ', 'confront', 'bet', 'techniqu', 'stunt', 'site', 'servic', 'recal', 'gotten', 'command', 'cabl', 'belong', 'freak', 'foot', 'stock', 'jimmi', 'fu', 'capabl', 'bug', 'bright', 'african', 'succeed', 'fat', 'clark', 'whilst', 'structur', 'spanish', 'gene', 'boat', 'presid', 'paper', 'tree', 'kidnap', 'factor', 'belief', 'witti', 'santa', 'realism', 'realis', 'educ', 'complic', 'bob', 'attend', 'smoke', 'finest', 'broken', 'assist', 'observ', 'determin', 'depart', 'rubbish', 'routin', 'oper', 'lewi', 'hat', 'domin', 'advanc', 'lone', 'kinda', 'hook', 'foreign', 'fame', 'below', 'safe', 'rank', 'numer', 'morgan', 'werewolf', 'washington', 'shape', 'shallow', 'rose', 'civil', 'ordinari', 'morn', 'gari', 'winner', 'vs', 'peac', 'kong', 'accomplish', 'whenev', 'virtual', 'offens', 'grab', 'luck', 'welcom', 'unfunni', 'patient', 'eric', 'contriv', 'con', 'complain', 'bigger', 'activ', 'trek', 'pretend', 'guard', 'dimension', 'lesbian', 'dri', 'code', 'cain', 'wake', 'statu', 'manipul', 'flash', 'dancer', 'corrupt', 'speech', 'sourc', 'signific', 'secur', 'gain', 'context', 'awkward', 'albert', 'v', 'sean', 'reli', 'psycho', 'clip', 'anthoni', 'theatric', 'religion', 'priest', 'corni', 'advic', 'w', 'flow', 'curiou', 'addict', 'specif', 'jennif', 'howard', 'h', 'golden', 'skin', 'promot', 'core', 'comfort', 'organ', 'luke', 'lucki', 'cheat', 'cash', 'asian', 'lower', 'dislik', 'associ', 'wing', 'spell', 'regret', 'frequent', 'frankli', 'devic', 'degre', 'contribut', 'balanc', 'sake', 'print', 'lake', 'forgiv', 'thoma', 'mass', 'betti', '13', 'unexpect', 'gordon', 'crack', 'unfold', 'depend', 'construct', 'categori', 'amateur', 'walter', 'mirror', 'matur', 'invit', 'intellectu', 'grown', 'grew', 'condit', 'anna', 'veteran', 'sudden', 'spectacular', 'sole', 'subtitl', 'robin', 'overli', 'meanwhil', 'liner', 'honor', 'grip', 'gift', 'freedom', 'experienc', 'demonstr', 'card', 'unabl', 'theori', 'sheriff', 'section']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As we have seen first 54000 terms are very sparce and they won't help our classification\n",
    "#so we filter them out\n",
    "terms = [tf_term[-(i + 1)][1] for i in range(2000)]\n",
    "print(terms)\n",
    "\n",
    "del tf_term\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "X0GIj2k5zw6P"
   },
   "outputs": [],
   "source": [
    "# Now before implementing tf-idfs we first need\n",
    "# a hash function to get termID from each term\n",
    "all_tokens_hash = len(terms) * ['#']\n",
    "\n",
    "def hash_func(token):\n",
    "    B = 256\n",
    "    M = len(terms)\n",
    "    cur = 0\n",
    "    for i in range(len(token)):\n",
    "        cur = ((cur * B) + ord(token[i])) % M\n",
    "    while (all_tokens_hash[cur] != token and all_tokens_hash[cur] != '#'):\n",
    "        cur = (cur + 1) % M\n",
    "    if (all_tokens_hash[cur] == '#'):\n",
    "        all_tokens_hash[cur] = token\n",
    "    return cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gcLUbsnU1WR4"
   },
   "outputs": [],
   "source": [
    "#Now it's time to build the tf-idf matrice for our train and test data\n",
    "\n",
    "#first we build the tf matrice\n",
    "tf_idf_mat_train = [[0 for i in range(len(terms))] for j in range(df_train.shape[0])]\n",
    "tf_idf_mat_test = [[0 for i in range(len(terms))] for j in range(df_test.shape[0])]\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "  tokenized_document = tokenizer(df_train.iloc[i]['text'])\n",
    "  for token in tokenized_document:\n",
    "    if token not in terms:\n",
    "      continue\n",
    "    tf_idf_mat_train[i][hash_func(token)] += 1\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "  tokenized_document = tokenizer(df_test.iloc[i]['text'])\n",
    "  for token in tokenized_document:\n",
    "    if token not in terms:\n",
    "      continue\n",
    "    tf_idf_mat_test[i][hash_func(token)] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BMkumRJd2ulx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0     1     2     3     4         5     6     7     8     9     ...  \\\n",
      "0       0.0   0.0   0.0   0.0   0.0  6.801796   0.0   0.0   0.0   0.0  ...   \n",
      "1       0.0   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  ...   \n",
      "2       0.0   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  ...   \n",
      "3       0.0   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  ...   \n",
      "4       0.0   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  ...   \n",
      "...     ...   ...   ...   ...   ...       ...   ...   ...   ...   ...  ...   \n",
      "24995   0.0   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  ...   \n",
      "24996   0.0   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  ...   \n",
      "24997   0.0   0.0   0.0   0.0   0.0  3.400898   0.0   0.0   0.0   0.0  ...   \n",
      "24998   0.0   0.0   0.0   0.0   0.0  3.400898   0.0   0.0   0.0   0.0  ...   \n",
      "24999   0.0   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  ...   \n",
      "\n",
      "             1990       1991  1992  1993  1994  1995  1996  1997  1998  1999  \n",
      "0        0.000000   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "1        0.000000   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "2        0.000000   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "3        0.000000   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "4        0.000000   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "...           ...        ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "24995  111.111111   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "24996    0.000000   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "24997    0.000000  13.781698   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "24998    0.000000   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "24999    0.000000   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[25000 rows x 2000 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second we calculate idf for each term and multipy each tf matrice row by its value\n",
    "#and at last we save it as a dataframe\n",
    "\n",
    "N = df_train.shape[0]\n",
    "idf_mat = [N / sum([(tf_idf_mat_train[j][i] > 0) for j in range(df_train.shape[0])])\n",
    " for i in range(len(terms))]\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "  for j in range(len(terms)):\n",
    "    tf_idf_mat_train[i][j] *= idf_mat[j]\n",
    "    \n",
    "for i in range(df_test.shape[0]):\n",
    "  for j in range(len(terms)):\n",
    "    tf_idf_mat_test[i][j] *= idf_mat[j]\n",
    "\n",
    "df_tf_idf_train = pd.DataFrame(tf_idf_mat_train)\n",
    "df_tf_idf_train.to_csv('df_tf_idf_train.csv', index=False)\n",
    "\n",
    "df_tf_idf_test = pd.DataFrame(tf_idf_mat_test)\n",
    "df_tf_idf_test.to_csv('df_tf_idf_test.csv', index=False)\n",
    "\n",
    "print(df_tf_idf_train)\n",
    "\n",
    "del tf_idf_mat_train\n",
    "del tf_idf_mat_test\n",
    "del idf_mat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pgCIUQT2979v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.76\n",
      "Precision train: 0.84\n",
      "Recall train: 0.64\n",
      "F1-score train: 0.73\n",
      "Accuracy test: 0.72\n",
      "Precision test: 0.81\n",
      "Recall test: 0.58\n",
      "F1-score test: 0.68\n"
     ]
    }
   ],
   "source": [
    "#Now its time to do our naiive bayes algorithm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_train = df_train['sentiment']\n",
    "y_test = df_test['sentiment']\n",
    "\n",
    "# Creating a Gaussian Naive Bayes classifier\n",
    "NB = GaussianNB()\n",
    "\n",
    "NB.fit(df_tf_idf_train, y_train)\n",
    "\n",
    "y_pred_train = NB.predict(df_tf_idf_train)\n",
    "y_pred_test = NB.predict(df_tf_idf_test)\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "f1_score_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Accuracy train: {accuracy_train:.2f}\")\n",
    "print(f\"Precision train: {precision_train:.2f}\")\n",
    "print(f\"Recall train: {recall_train:.2f}\")\n",
    "print(f\"F1-score train: {f1_score_train:.2f}\")\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy test: {accuracy_test:.2f}\")\n",
    "print(f\"Precision test: {precision_test:.2f}\")\n",
    "print(f\"Recall test: {recall_test:.2f}\")\n",
    "print(f\"F1-score test: {f1_score_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_tf_idf_train\n",
    "del df_tf_idf_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40693357, 48275570)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we learn our W2V model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenized_documents = [tokenizer(df_train.iloc[i]['text']) for i in range(df_train.shape[0])]\n",
    "model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, sg=0, min_count=1)\n",
    "model.train(tokenized_documents, total_examples=len(tokenized_documents), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n",
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "#Now we get vectors of all of our terms and set their mean to be our representive vector\n",
    "import numpy as np\n",
    "\n",
    "vw_embeded_train = np.empty(shape=(0, 100))\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    tokenized_document = tokenizer(df_train.iloc[i]['text'])\n",
    "    cur = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for token in tokenized_document:\n",
    "        if token not in terms:\n",
    "            continue\n",
    "        \n",
    "        cnt += 1\n",
    "        cur += model.wv[token]\n",
    "        \n",
    "    if(cnt != 0):\n",
    "        cur /= cnt\n",
    "    \n",
    "    vw_embeded_train = np.concatenate((vw_embeded_train, [cur]))\n",
    "    \n",
    "vw_embeded_test = np.empty(shape=(0, 100))\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "    tokenized_document = tokenizer(df_test.iloc[i]['text'])\n",
    "    cur = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for token in tokenized_document:\n",
    "        if token not in terms:\n",
    "            continue\n",
    "        \n",
    "        cnt += 1\n",
    "        cur += model.wv[token]\n",
    "        \n",
    "    if(cnt != 0):\n",
    "        cur /= cnt\n",
    "    \n",
    "    vw_embeded_test = np.concatenate((vw_embeded_test, [cur]))\n",
    "\n",
    "print(vw_embeded_train.shape)\n",
    "print(vw_embeded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.83\n",
      "Precision train: 0.83\n",
      "Recall train: 0.84\n",
      "F1-score train: 0.83\n",
      "Accuracy test: 0.83\n",
      "Precision test: 0.83\n",
      "Recall test: 0.82\n",
      "F1-score test: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Now its time to train our svm model on our W2V data\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "svm.fit(vw_embeded_train, y_train)\n",
    "\n",
    "y_pred_train = svm.predict(vw_embeded_train)\n",
    "y_pred_test = svm.predict(vw_embeded_test)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "f1_score_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Accuracy train: {accuracy_train:.2f}\")\n",
    "print(f\"Precision train: {precision_train:.2f}\")\n",
    "print(f\"Recall train: {recall_train:.2f}\")\n",
    "print(f\"F1-score train: {f1_score_train:.2f}\")\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy test: {accuracy_test:.2f}\")\n",
    "print(f\"Precision test: {precision_test:.2f}\")\n",
    "print(f\"Recall test: {recall_test:.2f}\")\n",
    "print(f\"F1-score test: {f1_score_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we use LSA for our second embedding\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "lsa = TruncatedSVD(n_components=30)\n",
    "\n",
    "vw_train_lsa = lsa.fit_transform(vw_embeded_train)\n",
    "vw_test_lsa = lsa.transform(vw_embeded_test)\n",
    "\n",
    "del vw_embeded_train\n",
    "del vw_embeded_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.81\n",
      "Precision train: 0.81\n",
      "Recall train: 0.82\n",
      "F1-score train: 0.81\n",
      "Accuracy test: 0.81\n",
      "Precision test: 0.82\n",
      "Recall test: 0.81\n",
      "F1-score test: 0.81\n"
     ]
    }
   ],
   "source": [
    "#Now its time to train our svm model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "svm.fit(vw_train_lsa, y_train)\n",
    "\n",
    "y_pred_train = svm.predict(vw_train_lsa)\n",
    "y_pred_test = svm.predict(vw_test_lsa)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "f1_score_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Accuracy train: {accuracy_train:.2f}\")\n",
    "print(f\"Precision train: {precision_train:.2f}\")\n",
    "print(f\"Recall train: {recall_train:.2f}\")\n",
    "print(f\"F1-score train: {f1_score_train:.2f}\")\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy test: {accuracy_test:.2f}\")\n",
    "print(f\"Precision test: {precision_test:.2f}\")\n",
    "print(f\"Recall test: {recall_test:.2f}\")\n",
    "print(f\"F1-score test: {f1_score_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "#First we load our GloVe model\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "(25000, 300)\n"
     ]
    }
   ],
   "source": [
    "#And we get vectors of all of our terms and set their mean to be our representive vector\n",
    "GV_embeded_train = np.empty(shape=(0, 300))\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    tokenized_document = tokenizer(df_train.iloc[i]['text'])\n",
    "    cur = np.zeros(300)\n",
    "    cnt = 0\n",
    "    for token in tokenized_document:\n",
    "        if token not in glove_model.key_to_index:\n",
    "            continue\n",
    "        \n",
    "        cnt += 1\n",
    "        cur += glove_model[token]\n",
    "        \n",
    "    if(cnt != 0):\n",
    "        cur /= cnt\n",
    "    \n",
    "    GV_embeded_train = np.concatenate((GV_embeded_train, [cur]))\n",
    "    \n",
    "GV_embeded_test = np.empty(shape=(0, 300))\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "    tokenized_document = tokenizer(df_test.iloc[i]['text'])\n",
    "    cur = np.zeros(300)\n",
    "    cnt = 0\n",
    "    for token in tokenized_document:\n",
    "        if token not in glove_model.key_to_index:\n",
    "            continue\n",
    "        \n",
    "        cnt += 1\n",
    "        cur += glove_model[token]\n",
    "        \n",
    "    if(cnt != 0):\n",
    "        cur /= cnt\n",
    "    \n",
    "    GV_embeded_test = np.concatenate((GV_embeded_test, [cur]))\n",
    "\n",
    "print(GV_embeded_train.shape)\n",
    "print(GV_embeded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.81\n",
      "Precision train: 0.80\n",
      "Recall train: 0.81\n",
      "F1-score train: 0.81\n",
      "Accuracy test: 0.80\n",
      "Precision test: 0.80\n",
      "Recall test: 0.80\n",
      "F1-score test: 0.80\n"
     ]
    }
   ],
   "source": [
    "#Now its time to train our svm model on our W2V data\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "svm.fit(GV_embeded_train, y_train)\n",
    "\n",
    "y_pred_train = svm.predict(GV_embeded_train)\n",
    "y_pred_test = svm.predict(GV_embeded_test)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "f1_score_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Accuracy train: {accuracy_train:.2f}\")\n",
    "print(f\"Precision train: {precision_train:.2f}\")\n",
    "print(f\"Recall train: {recall_train:.2f}\")\n",
    "print(f\"F1-score train: {f1_score_train:.2f}\")\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy test: {accuracy_test:.2f}\")\n",
    "print(f\"Precision test: {precision_test:.2f}\")\n",
    "print(f\"Recall test: {recall_test:.2f}\")\n",
    "print(f\"F1-score test: {f1_score_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2836"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del vw_train_lsa\n",
    "del vw_test_lsa\n",
    "del GV_embeded_train\n",
    "del GV_embeded_test\n",
    "del glove_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we train our FastText model\n",
    "from gensim.models import FastText\n",
    "\n",
    "tokenized_documents = [tokenizer(df_train.iloc[i]['text']) for i in range(df_train.shape[0])]\n",
    "fasttext_model = FastText(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n",
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "#And we get vectors of all of our terms and set their mean to be our representive vector\n",
    "FT_embeded_train = np.empty(shape=(0, 100))\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    tokenized_document = tokenizer(df_train.iloc[i]['text'])\n",
    "    cur = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for token in tokenized_document:\n",
    "        if token not in terms:\n",
    "            continue\n",
    "        \n",
    "        cnt += 1\n",
    "        cur += fasttext_model.wv[token]\n",
    "        \n",
    "    if(cnt != 0):\n",
    "        cur /= cnt\n",
    "    \n",
    "    FT_embeded_train = np.concatenate((FT_embeded_train, [cur]))\n",
    "    \n",
    "FT_embeded_test = np.empty(shape=(0, 100))\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "    tokenized_document = tokenizer(df_test.iloc[i]['text'])\n",
    "    cur = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for token in tokenized_document:\n",
    "        if token not in terms:\n",
    "            continue\n",
    "        \n",
    "        cnt += 1\n",
    "        cur += fasttext_model.wv[token]\n",
    "        \n",
    "    if(cnt != 0):\n",
    "        cur /= cnt\n",
    "    \n",
    "    FT_embeded_test = np.concatenate((FT_embeded_test, [cur]))\n",
    "\n",
    "print(FT_embeded_train.shape)\n",
    "print(FT_embeded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.81\n",
      "Precision train: 0.80\n",
      "Recall train: 0.81\n",
      "F1-score train: 0.81\n",
      "Accuracy test: 0.80\n",
      "Precision test: 0.81\n",
      "Recall test: 0.80\n",
      "F1-score test: 0.80\n"
     ]
    }
   ],
   "source": [
    "#Now its time to train our svm model on our W2V data\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "svm.fit(FT_embeded_train, y_train)\n",
    "\n",
    "y_pred_train = svm.predict(FT_embeded_train)\n",
    "y_pred_test = svm.predict(FT_embeded_test)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "f1_score_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Accuracy train: {accuracy_train:.2f}\")\n",
    "print(f\"Precision train: {precision_train:.2f}\")\n",
    "print(f\"Recall train: {recall_train:.2f}\")\n",
    "print(f\"F1-score train: {f1_score_train:.2f}\")\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy test: {accuracy_test:.2f}\")\n",
    "print(f\"Precision test: {precision_test:.2f}\")\n",
    "print(f\"Recall test: {recall_test:.2f}\")\n",
    "print(f\"F1-score test: {f1_score_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLszricT8zvMNmv1W/YbnE",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
